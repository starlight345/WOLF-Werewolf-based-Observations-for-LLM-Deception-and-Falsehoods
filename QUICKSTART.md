# Quick Start Guide: WOLF with Activation Logging

## Step 1: Install Dependencies

```bash
cd /home/sychoe/WOLF-Werewolf-based-Observations-for-LLM-Deception-and-Falsehoods

# Install all dependencies
pip install -r requirements.txt

# Login to Hugging Face (required for LLaMA models)
huggingface-cli login
```

## Step 2: Configure Your Model

Edit `config.py` if you want to customize:
- Model path (default: `meta-llama/Llama-3.1-8B-Instruct`)
- Activation logging settings
- Device (`cuda` or `cpu`)
- Quantization options

## Step 3: Run a Game

### Option A: Full Precision LLaMA (requires ~16GB GPU memory)

```bash
python run.py --model llama-3.1-8b --log-activations
```

### Option B: 4-bit Quantized LLaMA (requires ~8GB GPU memory)

```bash
python run.py --model llama-3.1-8b-4bit --log-activations
```

### Option C: Use OpenAI API (original behavior, no activations)

```bash
export OPENAI_API_KEY="your-key-here"
python run.py --model gpt-4o
```

## Step 4: Check the Output

After the game completes, you'll see:

```
Log Files
  Events (NDJSON): ./logs/TIMESTAMP/events.ndjson
  Final State JSON: ./logs/TIMESTAMP/game_state.json
  Final Metrics JSON: ./logs/TIMESTAMP/final_metrics.json
  Run Metadata: ./logs/TIMESTAMP/run_meta.json
  Activations Directory: ./logs/TIMESTAMP/activations
```

## Step 5: Analyze Activations

```bash
# Run analysis script
python analyze_activations.py \
  --run-dir logs/TIMESTAMP \
  --visualize \
  --save-primitives
```

This will:
- Compute contrastive directions between deceptive/truthful statements
- Calculate layer-wise separation metrics
- Generate visualization plots
- Save representation primitives for downstream use

## Step 6: Load and Inspect Data

### Python Script Example

```python
import numpy as np
import json
from pathlib import Path

# Load events
run_dir = Path("logs/TIMESTAMP")
events = []
with open(run_dir / "events.ndjson") as f:
    for line in f:
        events.append(json.loads(line))

# Find debate events with activations
debate_events = [e for e in events if e["event"] == "debate" and "activation_file" in e]

print(f"Found {len(debate_events)} debate statements with activations")

# Load first activation
first_event = debate_events[0]
act_file = first_event["activation_file"]
act_path = run_dir / "activations" / act_file

act_data = np.load(act_path)
hidden_states = act_data["hidden_states"]  # [n_layers, hidden_dim]

print(f"Activation shape: {hidden_states.shape}")
print(f"Layer indices: {act_data['layer_indices']}")
print(f"Reduction method: {act_data['reduction']}")

# Get statement and deception label
statement = first_event["details"]["dialogue"]
is_deceptive = first_event["details"]["raw_output"].get("is_deceptive", False)

print(f"\nStatement: {statement}")
print(f"Is deceptive: {is_deceptive}")
```

## Expected Output Structure

```
logs/
  20260130-153045-123456/          # Timestamp-based run ID
    events.ndjson                   # ~1-5 MB (text events)
    game_state.json                 # Full final state
    final_metrics.json              # Aggregated metrics
    run_meta.json                   # Run metadata
    activations/                    # Activation files
      001_003_Alice_debate.npz      # ~500 KB per file (float16 compressed)
      001_004_Bob_debate.npz
      001_005_Charlie_debate.npz
      ...
    layer_separation.png            # Generated by analyze_activations.py
    representation_primitives.npz   # Extracted primitives
```

## Troubleshooting

### GPU Out of Memory

```bash
# Use 4-bit quantization
python run.py --model llama-3.1-8b-4bit

# Or reduce the number of layers captured
# Edit config.py: ACTIVATION_CONFIG["layers"] = [0, 8, 16, 24, 31]
```

### Model Download Issues

```bash
# Ensure you're logged in to Hugging Face
huggingface-cli login

# Verify access to LLaMA model on HF website
# https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
```

### Import Errors

```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Check CUDA version
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"
```

## Next Steps

1. **Extract Primitives**: Use `analyze_activations.py` to identify contrastive directions
2. **Test Necessity**: Implement intervention hooks to ablate primitives
3. **Test Sufficiency**: Inject primitives and measure behavioral changes
4. **Publish Results**: Document your findings and cite the work!

## Configuration Cheat Sheet

Edit `config.py`:

```python
# Use different LLaMA model
AVAILABLE_MODELS["my-llama"] = {
    "name": "meta-llama/Llama-2-7b-chat-hf",  # Any HF model path
    "backend": "llama_hf",
    "device": "cuda",
    "dtype": "float16",
}

# Capture only specific layers
ACTIVATION_CONFIG = {
    "enabled": True,
    "layers": [0, 8, 16, 24, 31],  # 5 layers instead of all
    "reduction": "last_token",     # or "mean_pool"
}

# Use different default model
DEFAULT_MODEL = "my-llama"
```

## Performance Benchmarks

On a single A100 GPU (40GB):

| Model | Precision | Memory | Time/Turn | Activation Size |
|-------|-----------|--------|-----------|-----------------|
| LLaMA 3.1 8B | float16 | ~16 GB | ~3s | ~500 KB/file |
| LLaMA 3.1 8B | 4-bit | ~8 GB | ~2s | ~500 KB/file |
| GPT-4o (API) | N/A | 0 GB | ~5s | N/A |

## Contact

For issues, questions, or contributions, please open an issue on GitHub or contact the authors.
